{
    "docs": [
        {
            "location": "/",
            "text": "Python, Data, Machine Learning, Raspberry Pi, and Distractions\n\n\nNEW:\n \nAnalysis of bicyle traffic across the four NYC East River bridges",
            "title": "Home"
        },
        {
            "location": "/#python-data-machine-learning-raspberry-pi-and-distractions",
            "text": "NEW:   Analysis of bicyle traffic across the four NYC East River bridges",
            "title": "Python, Data, Machine Learning, Raspberry Pi, and Distractions"
        },
        {
            "location": "/about/",
            "text": "My name is Mike Moran and this is a repository of my code snippets and projects I'm working on.  \n\n\ndistracteddata@gmail.com",
            "title": "About"
        },
        {
            "location": "/python/",
            "text": "",
            "title": "Python (coming soon)"
        },
        {
            "location": "/Pandas/pandas_excel_to_html/",
            "text": "Pandas Documentation\n\n\nimport pandas as pd\nfile_url = 'path_to_excel_file.xlsx'\ndf = pd.read_excel(file_url, sheetname='SheetX')\ndf.to_html('filename.html', justify='left')",
            "title": "Excel to HTML"
        },
        {
            "location": "/NLP/Scraping-Crawling-Multiprocessing/",
            "text": "Simple code snippets to crawl and scrape\n\n\nimport bs4\nimport requests\nfrom slugify import slugify\nfrom multiprocessing.dummy import Pool\n\n\n\n\nsources = [list of websites here]\n\n\n\n\ndef crawl(url):\n    domain = url.split(\"//www.\")[-1].split(\"/\")[0]\n    html = requests.get(url).content\n    soup = bs4.BeautifulSoup(html, \"lxml\")\n    links = set(soup.find_all('a', href=True))\n    for link in links:\n        sub_url = link['href']\n        page_name = link.string\n        if domain in sub_url:\n            try:\n                page = requests.get(sub_url).content\n                filename = slugify(page_name).lower() + '.html'\n                with open(filename, 'w') as f:\n                    f.write(page)\n            except:\n                pass\n\n\n\n\nif __name__ == '__main__':\n    for url in sources:\n        crawl(url)\n\n\n\n\nCrawling and scraping can consume a large amount of time. To speed the crawling and scraping, the multiprocessing library can be used. In my experience this can save considerable time. For example, using the above code took my machine 274 seconds to execute on 4 websites. However, using the above code and then spreading it across multiple processes with the below snippets decreased that execution time to 150 seconds.\n\n\ndef multi_proc_crawl(url_list, processes=2):\n    pool = Pool(processes)\n    pool.map(crawl, url_list)\n    pool.close()\n    pool.join()\n\n\n\n\nif __name__ == '__main__':\n    multi_proc_crawl(sources, 4)",
            "title": "Website scraping, crawling, and multiprocessing"
        },
        {
            "location": "/NLP/Scraping-Crawling-Multiprocessing/#simple-code-snippets-to-crawl-and-scrape",
            "text": "import bs4\nimport requests\nfrom slugify import slugify\nfrom multiprocessing.dummy import Pool  sources = [list of websites here]  def crawl(url):\n    domain = url.split(\"//www.\")[-1].split(\"/\")[0]\n    html = requests.get(url).content\n    soup = bs4.BeautifulSoup(html, \"lxml\")\n    links = set(soup.find_all('a', href=True))\n    for link in links:\n        sub_url = link['href']\n        page_name = link.string\n        if domain in sub_url:\n            try:\n                page = requests.get(sub_url).content\n                filename = slugify(page_name).lower() + '.html'\n                with open(filename, 'w') as f:\n                    f.write(page)\n            except:\n                pass  if __name__ == '__main__':\n    for url in sources:\n        crawl(url)  Crawling and scraping can consume a large amount of time. To speed the crawling and scraping, the multiprocessing library can be used. In my experience this can save considerable time. For example, using the above code took my machine 274 seconds to execute on 4 websites. However, using the above code and then spreading it across multiple processes with the below snippets decreased that execution time to 150 seconds.  def multi_proc_crawl(url_list, processes=2):\n    pool = Pool(processes)\n    pool.map(crawl, url_list)\n    pool.close()\n    pool.join()  if __name__ == '__main__':\n    multi_proc_crawl(sources, 4)",
            "title": "Simple code snippets to crawl and scrape"
        },
        {
            "location": "/machine_learning/",
            "text": "",
            "title": "Machine Learning (coming soon)"
        },
        {
            "location": "/data_visualization/",
            "text": "",
            "title": "Data Visualization (coming soon)"
        },
        {
            "location": "/Raspberry_Pi/setup/",
            "text": "These are my 'quick setup' notes for getting a fresh pi up and running\n\n\nRaspberry Pi Quick Setup:\n\n\n\n\nDownload \nNoobs\n\n\nDownload \nSD Card Formatter\n\n\nFormat SD Card (Don't use quick option), no Volume needed\n\n\nExtract Noobs zip and copy contents to SD card\n\n\nInsert SD card into Raspberry Pi\n\n\nPower up Raspberry Pi and install the Raspbian distro\n\n\nClick the wifi icon and connect to wireless\n\n\nsudo apt-get update\n\n\nsudo apt-get upgrade\n\n\nDon't forget to change the default password!: \npasswd\n\n\n\n\n\n\nRemove Wolfram Whatever\n\n\n\n\nsudo apt-get purge wolfram-engine\n\n\nsudo apt-get autoremove\n\n\nsudo apt-get update\n\n\nsudo apt-get upgrade\n\n\n\n\n\n\nEnable SSH\n\n\n\n\nifconfig\n - retrieve IP address\n\n\nsudo raspi-config\n\n\nInterfacing Options\n > \nEnable SSH\n\n\nDon't forget to change wifi password from default:\n \npasswd\n\n\n\n\n\n\nEnable Remote Desktop\n\n\n\n\nsudo apt-get install tightvncserver\n\n\nsudo apt-get install xrdp\n\n\n\n\n\n\nInstall Miniconda\n\n\n\n\nDownload \nminiconda3-latest-Linux-armv7l.sh\n\n\nsudo bash miniconda3-latest-Linux-armv7l.sh\n\n\nconda install pandas\n\n\nconda install ipython\n\n\nconda install nltk\n\n\nconda install flask\n\n\nconda install scikit-learn",
            "title": "Setup"
        },
        {
            "location": "/Raspberry_Pi/setup/#raspberry-pi-quick-setup",
            "text": "Download  Noobs  Download  SD Card Formatter  Format SD Card (Don't use quick option), no Volume needed  Extract Noobs zip and copy contents to SD card  Insert SD card into Raspberry Pi  Power up Raspberry Pi and install the Raspbian distro  Click the wifi icon and connect to wireless  sudo apt-get update  sudo apt-get upgrade  Don't forget to change the default password!:  passwd",
            "title": "Raspberry Pi Quick Setup:"
        },
        {
            "location": "/Raspberry_Pi/setup/#remove-wolfram-whatever",
            "text": "sudo apt-get purge wolfram-engine  sudo apt-get autoremove  sudo apt-get update  sudo apt-get upgrade",
            "title": "Remove Wolfram Whatever"
        },
        {
            "location": "/Raspberry_Pi/setup/#enable-ssh",
            "text": "ifconfig  - retrieve IP address  sudo raspi-config  Interfacing Options  >  Enable SSH  Don't forget to change wifi password from default:   passwd",
            "title": "Enable SSH"
        },
        {
            "location": "/Raspberry_Pi/setup/#enable-remote-desktop",
            "text": "sudo apt-get install tightvncserver  sudo apt-get install xrdp",
            "title": "Enable Remote Desktop"
        },
        {
            "location": "/Raspberry_Pi/setup/#install-miniconda",
            "text": "Download  miniconda3-latest-Linux-armv7l.sh  sudo bash miniconda3-latest-Linux-armv7l.sh  conda install pandas  conda install ipython  conda install nltk  conda install flask  conda install scikit-learn",
            "title": "Install Miniconda"
        },
        {
            "location": "/Raspberry_Pi/picam/",
            "text": "Setup\n\n\n\n\nEnable the camera via \nsudo raspi-config\n\n\nreboot\n\n\n\n\nCommand line Usage\n\n\nraspistill -o image.jpg\n\n\nPython library Installation\n\n\n\n\nsudo apt-get update\n\n\nsudo apt-get install python-picamera\n\n\n\n\nUsage\n\n\nimport picamera\n\n\nCreate an instance of the PiCamera class:\n \n\n\ncam = picamera.PiCamera\n \n\n\nTake a photo:\n\n\ncam.capture('photo.jpg')",
            "title": "Pi Camera"
        },
        {
            "location": "/Raspberry_Pi/picam/#setup",
            "text": "Enable the camera via  sudo raspi-config  reboot",
            "title": "Setup"
        },
        {
            "location": "/Raspberry_Pi/picam/#command-line-usage",
            "text": "raspistill -o image.jpg",
            "title": "Command line Usage"
        },
        {
            "location": "/Raspberry_Pi/picam/#python-library-installation",
            "text": "sudo apt-get update  sudo apt-get install python-picamera",
            "title": "Python library Installation"
        },
        {
            "location": "/Raspberry_Pi/picam/#usage",
            "text": "import picamera  Create an instance of the PiCamera class:    cam = picamera.PiCamera    Take a photo:  cam.capture('photo.jpg')",
            "title": "Usage"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/",
            "text": "New York City has quite a few interesting data sets. Earlier this year, I stayed in a hotel in Manhattan near the Williamsburg Bridge. It's an amazing site to see the number of cyclists commuting every morning there. This project takes the data collected by NYC DOT from April through October 2016 and examines some of the interesting data points.\n\n\nAbout the data\n\n\nThis data is available on NYC's \nOpenData\n website. Sweet. That was easy! Of course, nothing in data analysis is ever easy, and this data is no different. What the folks at NYC DOT went ahead and did was to place each month's collected data in a formatted Excel sheet and also add some analysis in there for good measure. Awesome!\n\n\n\n\nSo not only do we have data that is in a messy spreadsheet, it's across multiple spreadsheets - one for each month!\n\n\nSetup\n\n\nAs for most projects that poke at these sorts of data sets, I'm using a Jupyter Notebook. I've extracted all of the downloaded Excel files to a directory and created a notebook in the same directory.\n\n\nReading the data\n\n\nOur first chore is munging the data. Some folks hate this part of analysis, but it's rare that one does not have to clean the data, so why not embrace it?!\n\n\nImport the libraries we'll need\n\n\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\n\nList the contents of our working directory\n\n\npath = os.getcwd()\nfiles = os.listdir(path)\nfiles\n\n\n\n\nThis should yield something like:\n\n\n['05 May 2016 Cyclist Numbers for Web.xlsx',\n 'Bicycle Counts for East River Bridges Metadata.docx',\n '07 July 2016 Cyclist Numbers for Web.xlsx',\n 'NYC East River Bicycle Analysis.ipynb',\n '08 August 2016 Cyclist Numbers for Web.xlsx',\n '06 June 2016 Cyclist Numbers for Web.xlsx',\n '09 September 2016 Cyclist Numbers for Web.xlsx',\n '10 October 2016 Cyclist Numbers for Web.xlsx',\n '04 April 2016 Cyclist Numbers for Web.xlsx',\n '.ipynb_checkpoints']\n\n\n\n\nObtain a list of the file names\n\nThe above is a nice list, but it contains files other than our Excel file. We'll use a list comprehension to iterate over the files in the current working directory to return a list of only files witht he \".xlsx\" extension.\n\n\nxlsx_files = [f for f in files if f[-4:] == 'xlsx']\nxlsx_files.sort() # for my ocd\nxlsx_files\n\n\n\n\nAnd now we have:\n\n\n['04 April 2016 Cyclist Numbers for Web.xlsx',\n '05 May 2016 Cyclist Numbers for Web.xlsx',\n '06 June 2016 Cyclist Numbers for Web.xlsx',\n '07 July 2016 Cyclist Numbers for Web.xlsx',\n '08 August 2016 Cyclist Numbers for Web.xlsx',\n '09 September 2016 Cyclist Numbers for Web.xlsx',\n '10 October 2016 Cyclist Numbers for Web.xlsx']\n\n\n\n\nShoving the data into a Pandas Dataframe\n\nSo we have our list of files. Now it's time to create a Pandas Dataframe, iterate over each file, and add the content to the dataframe. If you recall from the view of the spreadsheet, these files have other information that is not raw data in them. We'll use some of Pandas built-in features to help us import only the rows we want.\n\n\ndf = pd.DataFrame()\nfor f in xlsx_files:\n    data = pd.read_excel(f, header=5, index=1, parse_cols='B:K', skip_footer=8)\n    df = df.append(data)\n\n\n\n\nLet's do a sense check on our data\n\n\ndf.head()\n\n\n\n\ndf.shape\n(214, 10)\n\n\n\n\nSo the headers and first few rows look good. The shape of our dataframe shows the correct number of columns and about the right number of rows, but let's just go ahead and count the days of the months being analyzed as a little safety check:  \n\n\nApril - 30 days\nMay - 31 days\nJune - 30 days\nJuly - 31 days\nAugust - 31 days\nSeptember - 30 days\nOctober - 31 days\nTOTAL: 214 days \n\n\n\n\nSweet! I'd say we are in business! One quick observation of the first few rows shows a couple of surprises. We can see (without even inspecting data types) that both the 'Brooklyn Bridge' and 'Total' columns are floats, even though we would expect integer types. Let's cast those as ints now so our data behaves as we would expect.\n\n\ndf['Brooklyn Bridge'] = df['Brooklyn Bridge'].astype(int)\ndf['Total'] = df['Total'].astype(int)\n\n\n\n\n\nOK. That looks better. That 'Precipitation' column is looking like it might be tricky as it's currently a string type, but we'll figure that out later.  \n\n\nAnalyzing total bridge traffic\n\n\nWith a tidy dataframe, we can now start poking at the data a bit. First question that came to my mind looking at the data was \"What is the breakdown of bicycles over the various bridges?\"  \n\n\nWe'll aggregate just the bridge columns into a new dataframe and sum the totals.\n\n\nbridges = df[['Brooklyn Bridge', 'Manhattan Bridge', 'Williamsburg Bridge', 'Queensboro Bridge']]\nbridge_totals = bridges.sum().values\n\n\n\n\nLet's plot the data:\n\n\n# Data to plot\nlabels = list(bridges.columns)\nsizes = bridge_totals\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nexplode = (0, 0, 0.1, 0)  # explode 1st slice\n\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()\n\n\n\n\n\nI wasn't exactly surprised by this chart. The Williamsburg bridge is certainly very busy! Also, for anyone that's ever walked or biked across the Brooklyn Bridge - it's understandable why the numbers aren't as high!\n\n\nLooking at ridership by day of week\n\n\nSomething that might be interesting would be ridership totals over all four bridges by day of the week.\nI'm sure there's a better way to do this, but when I examine datetime type of data, I will usually create a new column with the data that I seek for easy analysis. So in this case, I'll use Pandas built-in method of parsing the day of the week from datetime objects and add that to a new column.  \n\n\ndf['Day_Of_Week'] = df.Date.dt.dayofweek\n\n\n \n\n\n# add column for day of week. Monday=0, Sunday=6\nday_of_week_totals = df.groupby(['Day_Of_Week'])['Total'].sum()\nday_of_week_totals\n\n\n\n\nDay_Of_Week\n0    601202\n1    623466\n2    672666\n3    623433\n4    557519\n5    465018\n6    425205\n\n\n\n\nThe above groups by 'Day of Week' and then sums the total. Let's chart our data:\n\n\n# Data to plot\nfig = plt.figure(figsize=(10,5))\n\nlabels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\ny_pos = np.arange(len(labels))\ntotals = list(day_of_week_totals)\n\nplt.bar(y_pos, totals, align='center', alpha=0.5)\nplt.xticks(y_pos, labels)\nplt.ylabel('# of rides')\n\nplt.show()\n\n\n\n\n\n\nPerhaps, from the data above, we might surmise that much of the bicyle traffic is communing to and from work.\n\n\nRidership by month of year\n\n\nLike above, we'll create a new column that contains the month of the year, group the data by that and sum the totals.\n\n\ndf['Month'] = df.Date.dt.month\nmonthly_totals = df.groupby(['Month'])['Total'].sum()\n\n# Data to plot\nfig = plt.figure(figsize=(10,5))\n\nlabels = ['APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT']\ny_pos = np.arange(len(labels))\ntotals = list(monthly_totals)\n\nplt.bar(y_pos, totals, align='center', alpha=0.5, color='g')\nplt.xticks(y_pos, labels)\nplt.ylabel('# of rides')\n\nplt.show()\n\n\n\n\n\n\nHow tough are NYC East River bridge cyclists?\n\n\nLet's determine if precipitation affects ridership. As noted previously, the 'Precipitation' column is not without drama. An 's' was used by the data collectors to indicate when the precipitation involved snow. Thank makes our analysis a bit tricky. So we'll make a judgement call and say either there was precipitation or not.\n\n\nsunshine = df.loc[df['Precipitation'] == 0]\nsunshine_total = sunshine['Total'].sum()\n\nits_raining = df.loc[df['Precipitation'] != 0]\nits_raining_total = its_raining['Total'].sum()\n\n# Data to plot\nlabels = 'Sunny(ish)', \"It's Raining/Snowing\"\ntotals = [sunshine_total, its_raining_total]\ncolors = ['gold', 'lightblue']\nexplode = (0, 0.1)  # explode 1st slice\n\n# Plot\nplt.pie(totals, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=100)\n\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\nI don't think the above is a surprise - no disrespect NYC cyclists! I hope you enjoyed exploring this data!",
            "title": "NYC East River Bridges Bicyle Traffic Analysis"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#about-the-data",
            "text": "This data is available on NYC's  OpenData  website. Sweet. That was easy! Of course, nothing in data analysis is ever easy, and this data is no different. What the folks at NYC DOT went ahead and did was to place each month's collected data in a formatted Excel sheet and also add some analysis in there for good measure. Awesome!   So not only do we have data that is in a messy spreadsheet, it's across multiple spreadsheets - one for each month!",
            "title": "About the data"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#setup",
            "text": "As for most projects that poke at these sorts of data sets, I'm using a Jupyter Notebook. I've extracted all of the downloaded Excel files to a directory and created a notebook in the same directory.",
            "title": "Setup"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#reading-the-data",
            "text": "Our first chore is munging the data. Some folks hate this part of analysis, but it's rare that one does not have to clean the data, so why not embrace it?!  Import the libraries we'll need  import pandas as pd\nimport os\nfrom matplotlib import pyplot as plt\nimport numpy as np  List the contents of our working directory  path = os.getcwd()\nfiles = os.listdir(path)\nfiles  This should yield something like:  ['05 May 2016 Cyclist Numbers for Web.xlsx',\n 'Bicycle Counts for East River Bridges Metadata.docx',\n '07 July 2016 Cyclist Numbers for Web.xlsx',\n 'NYC East River Bicycle Analysis.ipynb',\n '08 August 2016 Cyclist Numbers for Web.xlsx',\n '06 June 2016 Cyclist Numbers for Web.xlsx',\n '09 September 2016 Cyclist Numbers for Web.xlsx',\n '10 October 2016 Cyclist Numbers for Web.xlsx',\n '04 April 2016 Cyclist Numbers for Web.xlsx',\n '.ipynb_checkpoints']  Obtain a list of the file names \nThe above is a nice list, but it contains files other than our Excel file. We'll use a list comprehension to iterate over the files in the current working directory to return a list of only files witht he \".xlsx\" extension.  xlsx_files = [f for f in files if f[-4:] == 'xlsx']\nxlsx_files.sort() # for my ocd\nxlsx_files  And now we have:  ['04 April 2016 Cyclist Numbers for Web.xlsx',\n '05 May 2016 Cyclist Numbers for Web.xlsx',\n '06 June 2016 Cyclist Numbers for Web.xlsx',\n '07 July 2016 Cyclist Numbers for Web.xlsx',\n '08 August 2016 Cyclist Numbers for Web.xlsx',\n '09 September 2016 Cyclist Numbers for Web.xlsx',\n '10 October 2016 Cyclist Numbers for Web.xlsx']  Shoving the data into a Pandas Dataframe \nSo we have our list of files. Now it's time to create a Pandas Dataframe, iterate over each file, and add the content to the dataframe. If you recall from the view of the spreadsheet, these files have other information that is not raw data in them. We'll use some of Pandas built-in features to help us import only the rows we want.  df = pd.DataFrame()\nfor f in xlsx_files:\n    data = pd.read_excel(f, header=5, index=1, parse_cols='B:K', skip_footer=8)\n    df = df.append(data)",
            "title": "Reading the data"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#lets-do-a-sense-check-on-our-data",
            "text": "df.head()   df.shape\n(214, 10)  So the headers and first few rows look good. The shape of our dataframe shows the correct number of columns and about the right number of rows, but let's just go ahead and count the days of the months being analyzed as a little safety check:    April - 30 days\nMay - 31 days\nJune - 30 days\nJuly - 31 days\nAugust - 31 days\nSeptember - 30 days\nOctober - 31 days\nTOTAL: 214 days   Sweet! I'd say we are in business! One quick observation of the first few rows shows a couple of surprises. We can see (without even inspecting data types) that both the 'Brooklyn Bridge' and 'Total' columns are floats, even though we would expect integer types. Let's cast those as ints now so our data behaves as we would expect.  df['Brooklyn Bridge'] = df['Brooklyn Bridge'].astype(int)\ndf['Total'] = df['Total'].astype(int)  \nOK. That looks better. That 'Precipitation' column is looking like it might be tricky as it's currently a string type, but we'll figure that out later.",
            "title": "Let's do a sense check on our data"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#analyzing-total-bridge-traffic",
            "text": "With a tidy dataframe, we can now start poking at the data a bit. First question that came to my mind looking at the data was \"What is the breakdown of bicycles over the various bridges?\"    We'll aggregate just the bridge columns into a new dataframe and sum the totals.  bridges = df[['Brooklyn Bridge', 'Manhattan Bridge', 'Williamsburg Bridge', 'Queensboro Bridge']]\nbridge_totals = bridges.sum().values  Let's plot the data:  # Data to plot\nlabels = list(bridges.columns)\nsizes = bridge_totals\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nexplode = (0, 0, 0.1, 0)  # explode 1st slice\n\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()  \nI wasn't exactly surprised by this chart. The Williamsburg bridge is certainly very busy! Also, for anyone that's ever walked or biked across the Brooklyn Bridge - it's understandable why the numbers aren't as high!",
            "title": "Analyzing total bridge traffic"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#looking-at-ridership-by-day-of-week",
            "text": "Something that might be interesting would be ridership totals over all four bridges by day of the week.\nI'm sure there's a better way to do this, but when I examine datetime type of data, I will usually create a new column with the data that I seek for easy analysis. So in this case, I'll use Pandas built-in method of parsing the day of the week from datetime objects and add that to a new column.    df['Day_Of_Week'] = df.Date.dt.dayofweek     # add column for day of week. Monday=0, Sunday=6\nday_of_week_totals = df.groupby(['Day_Of_Week'])['Total'].sum()\nday_of_week_totals  Day_Of_Week\n0    601202\n1    623466\n2    672666\n3    623433\n4    557519\n5    465018\n6    425205  The above groups by 'Day of Week' and then sums the total. Let's chart our data:  # Data to plot\nfig = plt.figure(figsize=(10,5))\n\nlabels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\ny_pos = np.arange(len(labels))\ntotals = list(day_of_week_totals)\n\nplt.bar(y_pos, totals, align='center', alpha=0.5)\nplt.xticks(y_pos, labels)\nplt.ylabel('# of rides')\n\nplt.show()   Perhaps, from the data above, we might surmise that much of the bicyle traffic is communing to and from work.",
            "title": "Looking at ridership by day of week"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#ridership-by-month-of-year",
            "text": "Like above, we'll create a new column that contains the month of the year, group the data by that and sum the totals.  df['Month'] = df.Date.dt.month\nmonthly_totals = df.groupby(['Month'])['Total'].sum()\n\n# Data to plot\nfig = plt.figure(figsize=(10,5))\n\nlabels = ['APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT']\ny_pos = np.arange(len(labels))\ntotals = list(monthly_totals)\n\nplt.bar(y_pos, totals, align='center', alpha=0.5, color='g')\nplt.xticks(y_pos, labels)\nplt.ylabel('# of rides')\n\nplt.show()",
            "title": "Ridership by month of year"
        },
        {
            "location": "/Projects/NYC_East_River_Bicycle_Analysis/#how-tough-are-nyc-east-river-bridge-cyclists",
            "text": "Let's determine if precipitation affects ridership. As noted previously, the 'Precipitation' column is not without drama. An 's' was used by the data collectors to indicate when the precipitation involved snow. Thank makes our analysis a bit tricky. So we'll make a judgement call and say either there was precipitation or not.  sunshine = df.loc[df['Precipitation'] == 0]\nsunshine_total = sunshine['Total'].sum()\n\nits_raining = df.loc[df['Precipitation'] != 0]\nits_raining_total = its_raining['Total'].sum()\n\n# Data to plot\nlabels = 'Sunny(ish)', \"It's Raining/Snowing\"\ntotals = [sunshine_total, its_raining_total]\ncolors = ['gold', 'lightblue']\nexplode = (0, 0.1)  # explode 1st slice\n\n# Plot\nplt.pie(totals, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=100)\n\nplt.axis('equal')\nplt.show()   I don't think the above is a surprise - no disrespect NYC cyclists! I hope you enjoyed exploring this data!",
            "title": "How tough are NYC East River bridge cyclists?"
        },
        {
            "location": "/Projects/failed_banks/",
            "text": "coming soon",
            "title": "Failed Banks Analysis"
        },
        {
            "location": "/Projects/ge_corn/",
            "text": "coming soon",
            "title": "Genetically Engineered Corn"
        },
        {
            "location": "/links/",
            "text": "Python\n\n\n\n\nAnaconda\n\n\nPyData\n\n\n\n\nLibraries\n\n\n\n\nPandas\n\n\nMatplotlib\n\n\nBokeh\n\n\n\n\nMachine Learning\n\n\n\n\nscikit-learn\n\n\nTensorFlow\n\n\nTensorFlow for Poets",
            "title": "Links"
        },
        {
            "location": "/links/#python",
            "text": "Anaconda  PyData",
            "title": "Python"
        },
        {
            "location": "/links/#libraries",
            "text": "Pandas  Matplotlib  Bokeh",
            "title": "Libraries"
        },
        {
            "location": "/links/#machine-learning",
            "text": "scikit-learn  TensorFlow  TensorFlow for Poets",
            "title": "Machine Learning"
        }
    ]
}